{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57647c29-c060-4576-a4ed-8e1f4e19978b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U \\\n",
    "#     torch==2.3.0+cu121 \\\n",
    "#     torchvision==0.18.0+cu121 \\\n",
    "#     torchaudio==2.3.0+cu121 \\\n",
    "#     bitsandbytes==0.43.3 \\\n",
    "#     triton==2.3.0 \\\n",
    "#     peft==0.10.0 \\\n",
    "#     trl==0.9.6 \\\n",
    "#     transformers==4.37.2 \\\n",
    "#     accelerate==0.27.2 \\\n",
    "#     datasets==2.16.0 \\\n",
    "#     evaluate==0.4.2 \\\n",
    "#     tensorboard==2.20.0 \\\n",
    "#     scipy==1.11.4 \\\n",
    "#     pandas==2.1.4 \\\n",
    "#     tqdm==4.67.1 \\\n",
    "#     --extra-index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac2ef5ff-f0df-4f73-9c32-3bb7e854d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Llama-2 7B Fine-tuning with RAG-Augmented Context\n",
    "For Multiple Choice Question Answering\n",
    "\n",
    "Dataset format expected:\n",
    "{\n",
    "  \"question\": \"...\",\n",
    "  \"answerChoices\": \"(A)... (B)... (C)... (D)...\",\n",
    "  \"correctAnswer\": \"D\",\n",
    "  \"context\": \"Retrieved passage 1...\\n\\nRetrieved passage 2...\"\n",
    "}\n",
    "\n",
    "Files needed:\n",
    "- train_with_context.jsonl\n",
    "- test_with_context.jsonl  \n",
    "- val_with_context.jsonl\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 1: IMPORTS\n",
    "# ============================================================================\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4db9724c-dd6d-49c9-81b2-f2d23cbeef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: MODEL CONFIGURATION\n",
    "# ============================================================================\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"  # or \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "new_model = \"llama-2-7b-mcq-rag-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93493e94-b4bd-4c79-962e-45d1e7fc2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: DATASET FILES (CHANGED FOR RAG DATASETS)\n",
    "# ============================================================================\n",
    "# Dataset files (RAG-augmented with context field)\n",
    "train_file = \"train_fine_tune_with_context.jsonl\"\n",
    "val_file = \"valid_fine_tune_with_context.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd4c6c69-13e9-4a0b-9e6b-dcdac9ec5905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: TRAINING HYPERPARAMETERS (KEPT ORIGINAL)\n",
    "# ============================================================================\n",
    "local_rank = -1\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "weight_decay = 0.001\n",
    "num_train_epochs = 2\n",
    "max_steps = -1  # -1 means train for num_train_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d1b0b80-f168-4410-9e1b-fa7f07debece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: LORA PARAMETERS (KEPT ORIGINAL)\n",
    "# ============================================================================\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5377a799-2542-4c66-a579-1154a7f74826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: QUANTIZATION PARAMETERS (KEPT ORIGINAL)\n",
    "# ============================================================================\n",
    "use_4bit = True\n",
    "use_nested_quant = False\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "622d5a6f-beb7-43c8-8e70-87ab11b667b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: SEQUENCE AND TRAINING SETTINGS (KEPT ORIGINAL)\n",
    "# ============================================================================\n",
    "max_seq_length = 1024\n",
    "packing = False\n",
    "gradient_checkpointing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85f7c72b-ab6e-4c2e-a22e-0fb954de34b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: PRECISION SETTINGS (KEPT ORIGINAL)\n",
    "# ============================================================================\n",
    "fp16 = False\n",
    "bf16 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d92bcac-aa52-494e-a0af-0bfdb80e0134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: OPTIMIZER AND SCHEDULER (KEPT ORIGINAL)\n",
    "# ============================================================================\n",
    "optim = \"paged_adamw_32bit\"\n",
    "lr_scheduler_type = \"cosine\"\n",
    "warmup_ratio = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4e21d7c-8e5a-4aa5-a0f2-007b0df30f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: LOGGING AND SAVING (KEPT ORIGINAL)\n",
    "# ============================================================================\n",
    "group_by_length = True\n",
    "save_steps = 100\n",
    "logging_steps = 10\n",
    "output_dir = \"llama2-mcq-rag-finetuned\"\n",
    "report_to = \"tensorboard\"\n",
    "tb_log_dir = \"llama2-mcq-rag-finetuned/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94c07c03-cd0a-4aa0-9ac7-70afd17a211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: DEVICE CONFIGURATION (KEPT ORIGINAL)\n",
    "# ============================================================================\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a02e7f08-f01f-48d1-b51c-16f487dbb6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "# CELL 12: NEFTUNE (KEPT ORIGINAL)\n",
    "# ============================================================================\n",
    "use_neftune = True\n",
    "neftune_noise_alpha = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c40b623-7a39-4c8f-82e4-6cbdc11e9d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONFIGURATION LOADED\n",
      "================================================================================\n",
      "Model: meta-llama/Llama-2-7b-hf\n",
      "Train file: train_fine_tune_with_context.jsonl\n",
      "Val file: valid_fine_tune_with_context.jsonl\n",
      "Output dir: llama2-mcq-rag-finetuned\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: PRINT CONFIGURATION\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"CONFIGURATION LOADED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Train file: {train_file}\")\n",
    "print(f\"Val file: {val_file}\")\n",
    "print(f\"Output dir: {output_dir}\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dd8ef2b-0038-4484-b12f-c27d77554249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets from local JSONL files...\n",
      "✓ Train samples: 8653\n",
      "✓ Validation samples: 2528\n",
      "✓ Columns: ['question', 'answerChoices', 'correctAnswer', 'context']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 14: LOAD DATASETS (ADAPTED FOR RAG FILES)\n",
    "# ============================================================================\n",
    "print(\"Loading datasets from local JSONL files...\")\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files=train_file, split=\"train\")\n",
    "val_dataset = load_dataset(\"json\", data_files=val_file, split=\"train\")\n",
    "\n",
    "# Shuffle datasets\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "val_dataset = val_dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"✓ Train samples: {len(train_dataset)}\")\n",
    "print(f\"✓ Validation samples: {len(val_dataset)}\")\n",
    "print(f\"✓ Columns: {train_dataset.column_names}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "052bcd76-d2af-4cf5-b565-62daa98fce04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE DATA:\n",
      "================================================================================\n",
      "Context (first 200 chars): The water cycle continuously recycles Earths water. Condensation plays an important role in this cycle. Find condensation in the water cycle Figure 1.3. It changes water vapor in the atmosphere to liq...\n",
      "Question: Which statement about the water cycle is false?\n",
      "Answer Choices: (A) The water cycle is a global cycle.. (B) The water cycle takes place only on and above Earths surface.. (C) In the water cycle, water exists in three different states.. (D) Water cycle processes include condensation..\n",
      "Correct Answer: B\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 15: PRINT SAMPLE DATA (ADAPTED FOR 'context' FIELD)\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE DATA:\")\n",
    "print(\"=\"*80)\n",
    "sample = train_dataset[0]\n",
    "print(f\"Context (first 200 chars): {sample['context'][:200]}...\")\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Answer Choices: {sample['answerChoices']}\")\n",
    "print(f\"Correct Answer: {sample['correctAnswer']}\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ddd3325f-1509-488e-b05d-da1651ccfe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 16: LOAD MODEL AND TOKENIZER FUNCTION (KEPT ORIGINAL)\n",
    "# ============================================================================\n",
    "def load_model(model_name):\n",
    "    \"\"\"Load tokenizer and model with QLoRA configuration\"\"\"\n",
    "    \n",
    "    # Compute dtype\n",
    "    compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "    # Quantization Config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=use_4bit,\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=use_nested_quant,\n",
    "    )\n",
    "\n",
    "    # Check for bfloat16 support\n",
    "    if compute_dtype == torch.float16 and use_4bit:\n",
    "        major, _ = torch.cuda.get_device_capability()\n",
    "        if major >= 8:\n",
    "            print(\"=\" * 80)\n",
    "            print(\"Your GPU supports bfloat16, you can accelerate training with bf16=True\")\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "    print(f\"Loading model: {model_name}...\")\n",
    "    \n",
    "    # Load Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    # Load LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        r=lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    # Load Tokenizer\n",
    "    print(f\"Loading tokenizer: {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    print(\"✓ Model and tokenizer loaded successfully!\\n\")\n",
    "\n",
    "    return model, tokenizer, peft_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5926d4b6-63a0-48d1-813a-ba8cb3605a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16, you can accelerate training with bf16=True\n",
      "================================================================================\n",
      "Loading model: meta-llama/Llama-2-7b-hf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27648b59d394a1cb371ce56f662c698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: meta-llama/Llama-2-7b-hf...\n",
      "✓ Model and tokenizer loaded successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# CELL 17: LOAD MODEL\n",
    "# ============================================================================\n",
    "model, tokenizer, peft_config = load_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f3ec1de-cc74-4cd3-b1a2-a4e3f57dcdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 18: DATA FORMATTING FUNCTION (ADAPTED FOR 'context' FIELD)\n",
    "# ============================================================================\n",
    "def formatting_func(example):\n",
    "    \"\"\"\n",
    "    Format examples for Llama2 instruction fine-tuning\n",
    "    Using Llama2-chat format with system prompt\n",
    "    \n",
    "    KEY CHANGE: Uses 'context' field instead of 'Context'\n",
    "    \"\"\"\n",
    "    output_texts = []\n",
    "    \n",
    "    for i in range(len(example['question'])):\n",
    "        context = str(example[\"context\"][i])  # Changed from \"Context\" to \"context\"\n",
    "        options = str(example[\"answerChoices\"][i])\n",
    "        question = str(example[\"question\"][i])\n",
    "        answer = str(example[\"correctAnswer\"][i])\n",
    "        \n",
    "        # Llama2-chat format with system prompt\n",
    "        template = f\"\"\"<s>[INST]<<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answer must be exactly one letter corresponding to the correct option. Do not include any explanation, punctuation, or extra text.\n",
    "<</SYS>>\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Options: {options}\n",
    "\n",
    "Answer: [/INST]{answer}</s>\"\"\"\n",
    "    \n",
    "        \n",
    "        output_texts.append(template)\n",
    "    \n",
    "    return output_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28586d51-5dae-4b45-a422-de0c211f6edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data formatting and collator configured\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 19: DATA COLLATOR (KEPT ORIGINAL)\n",
    "# ============================================================================\n",
    "# This ensures we only compute loss on the answer portion, not the instruction\n",
    "response_template = \"[/INST]\"\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer.encode(response_template, add_special_tokens=False)[2:], \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"✓ Data formatting and collator configured\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6d64571-0ff7-45f3-8107-874986f65cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training arguments configured\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 20: TRAINING ARGUMENTS (KEPT ORIGINAL)\n",
    "# ============================================================================\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=report_to,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    auto_find_batch_size=True,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "print(\"✓ Training arguments configured\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6eea4cf6-9bbf-4652-91be-cd3b72f85c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SFTTrainer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da96807353a44579c65189ecc00d2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using SFTConfig (TRL 1.0.0+ compatible)\n",
      "✓ Trainer initialized successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 21: INITIALIZE TRAINER (KEPT ORIGINAL LOGIC)\n",
    "# ============================================================================\n",
    "print(\"Initializing SFTTrainer...\")\n",
    "\n",
    "try:\n",
    "    from trl import SFTConfig\n",
    "    \n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim=optim,\n",
    "        save_steps=save_steps,\n",
    "        logging_steps=logging_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        fp16=fp16,\n",
    "        bf16=bf16,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        max_steps=max_steps,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        group_by_length=group_by_length,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        report_to=report_to,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        auto_find_batch_size=True,\n",
    "        save_total_limit=2,\n",
    "        max_seq_length=max_seq_length,\n",
    "        packing=packing,\n",
    "        neftune_noise_alpha=neftune_noise_alpha if use_neftune else None,\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        peft_config=peft_config,\n",
    "        tokenizer=tokenizer,\n",
    "        args=sft_config,\n",
    "        data_collator=collator,\n",
    "        formatting_func=formatting_func,\n",
    "    )\n",
    "    print(\"✓ Using SFTConfig (TRL 1.0.0+ compatible)\")\n",
    "    \n",
    "except ImportError:\n",
    "    # Fallback for older TRL versions\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=packing,\n",
    "        data_collator=collator,\n",
    "        formatting_func=formatting_func,\n",
    "        neftune_noise_alpha=neftune_noise_alpha if use_neftune else None,\n",
    "    )\n",
    "    print(\"✓ Using legacy SFTTrainer initialization (pre-1.0.0)\")\n",
    "\n",
    "print(\"✓ Trainer initialized successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72d1bc80-9e5b-4a1b-9da5-cdc952e3d169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING TRAINING\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8654' max='8654' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8654/8654 5:11:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.095700</td>\n",
       "      <td>0.616330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.650122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8654, training_loss=0.5778177645450137, metrics={'train_runtime': 18713.2336, 'train_samples_per_second': 0.925, 'train_steps_per_second': 0.462, 'total_flos': 3.52288019874816e+17, 'train_loss': 0.5778177645450137, 'epoch': 2.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 22: START TRAINING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3997e5f-7511-4be1-b64a-e0554e131c8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINDING BEST CHECKPOINT\n",
      "================================================================================\n",
      "Found 2 checkpoints:\n",
      "\n",
      "  checkpoint-4327\n",
      "    Epoch: 1.0\n",
      "    Eval Loss: 0.6163302659988403\n",
      "\n",
      "  checkpoint-8654\n",
      "    Epoch: 2.0\n",
      "    Eval Loss: 0.6501219272613525\n",
      "\n",
      "================================================================================\n",
      "BEST CHECKPOINT IDENTIFIED\n",
      "================================================================================\n",
      "Checkpoint: checkpoint-4327\n",
      "Epoch: 1.0\n",
      "Eval Loss: 0.6163302659988403\n",
      "Path: llama2-mcq-rag-finetuned/checkpoint-4327\n",
      "================================================================================\n",
      "\n",
      "Copying best checkpoint to deployment directory...\n",
      "✓ Best checkpoint copied to: ./llama2-mcq-best\n",
      "✓ Metadata saved to ./llama2-mcq-best/best_model_info.json\n",
      "\n",
      "✓ Files in ./llama2-mcq-best:\n",
      "  - README.md (0.00 MB)\n",
      "  - tokenizer_config.json (0.00 MB)\n",
      "  - tokenizer.json (1.76 MB)\n",
      "  - special_tokens_map.json (0.00 MB)\n",
      "  - rng_state.pth (0.01 MB)\n",
      "  - optimizer.pt (256.08 MB)\n",
      "  - adapter_config.json (0.00 MB)\n",
      "  - best_model_info.json (0.00 MB)\n",
      "  - trainer_state.json (0.05 MB)\n",
      "  - adapter_model.safetensors (128.02 MB)\n",
      "  - training_args.bin (0.00 MB)\n",
      "  - scheduler.pt (0.00 MB)\n",
      "\n",
      "================================================================================\n",
      "SUCCESS!\n",
      "================================================================================\n",
      "✓ Best model saved at: ./llama2-mcq-best\n",
      "✓ Epoch: 1.0\n",
      "✓ Validation Loss: 0.6163\n",
      "\n",
      "================================================================================\n",
      "HOW TO USE:\n",
      "================================================================================\n",
      "Download the entire folder: ./llama2-mcq-best\n",
      "\n",
      "On your local machine:\n",
      "```python\n",
      "from peft import AutoPeftModelForCausalLM\n",
      "from transformers import AutoTokenizer\n",
      "\n",
      "# Load model\n",
      "model = AutoPeftModelForCausalLM.from_pretrained(\n",
      "    './llama2-mcq-best',\n",
      "    device_map='auto',\n",
      "    torch_dtype='auto'\n",
      ")\n",
      "\n",
      "# Load tokenizer\n",
      "tokenizer = AutoTokenizer.from_pretrained('./llama2-mcq-best')\n",
      "\n",
      "# Generate answer\n",
      "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
      "outputs = model.generate(**inputs, max_new_tokens=5)\n",
      "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
      "```\n",
      "================================================================================\n",
      "\n",
      "✓ Checkpoint recovery complete!\n",
      "✓ Ready to download: ./llama2-mcq-best\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fixed Best Checkpoint Recovery\n",
    "Handles 4-bit quantization issues when loading from checkpoint\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "OUTPUT_DIR = \"llama2-mcq-rag-finetuned\"  # Your training output directory\n",
    "BEST_MODEL_DIR = \"./llama2-mcq-best\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINDING BEST CHECKPOINT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# FIND BEST CHECKPOINT\n",
    "# ============================================================================\n",
    "checkpoints_info = []\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith(\"checkpoint-\")]\n",
    "    checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
    "    \n",
    "    print(f\"Found {len(checkpoints)} checkpoints:\\n\")\n",
    "    \n",
    "    for checkpoint in checkpoints:\n",
    "        checkpoint_path = os.path.join(OUTPUT_DIR, checkpoint)\n",
    "        trainer_state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "        \n",
    "        if os.path.exists(trainer_state_path):\n",
    "            with open(trainer_state_path) as f:\n",
    "                state = json.load(f)\n",
    "                \n",
    "                epoch = state.get(\"epoch\")\n",
    "                \n",
    "                # Find eval loss from log history\n",
    "                eval_loss = None\n",
    "                for log_entry in reversed(state.get(\"log_history\", [])):\n",
    "                    if \"eval_loss\" in log_entry:\n",
    "                        eval_loss = log_entry[\"eval_loss\"]\n",
    "                        break\n",
    "                \n",
    "                checkpoints_info.append({\n",
    "                    'checkpoint': checkpoint,\n",
    "                    'path': checkpoint_path,\n",
    "                    'epoch': epoch,\n",
    "                    'eval_loss': eval_loss,\n",
    "                    'step': int(checkpoint.split(\"-\")[1])\n",
    "                })\n",
    "                \n",
    "                print(f\"  {checkpoint}\")\n",
    "                print(f\"    Epoch: {epoch}\")\n",
    "                print(f\"    Eval Loss: {eval_loss}\")\n",
    "                print()\n",
    "\n",
    "# ============================================================================\n",
    "# IDENTIFY BEST CHECKPOINT\n",
    "# ============================================================================\n",
    "if checkpoints_info:\n",
    "    valid_checkpoints = [c for c in checkpoints_info if c['eval_loss'] is not None]\n",
    "    \n",
    "    if valid_checkpoints:\n",
    "        best_checkpoint = min(valid_checkpoints, key=lambda x: x['eval_loss'])\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"BEST CHECKPOINT IDENTIFIED\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Checkpoint: {best_checkpoint['checkpoint']}\")\n",
    "        print(f\"Epoch: {best_checkpoint['epoch']}\")\n",
    "        print(f\"Eval Loss: {best_checkpoint['eval_loss']}\")\n",
    "        print(f\"Path: {best_checkpoint['path']}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # SOLUTION: JUST COPY THE CHECKPOINT FOLDER\n",
    "        # The checkpoint already contains the adapter weights\n",
    "        # No need to load and re-save with AutoPeftModel\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"Copying best checkpoint to deployment directory...\")\n",
    "        \n",
    "        import shutil\n",
    "        \n",
    "        # Remove existing best model dir if it exists\n",
    "        if os.path.exists(BEST_MODEL_DIR):\n",
    "            shutil.rmtree(BEST_MODEL_DIR)\n",
    "        \n",
    "        # Copy the entire checkpoint folder\n",
    "        shutil.copytree(best_checkpoint['path'], BEST_MODEL_DIR)\n",
    "        \n",
    "        print(f\"✓ Best checkpoint copied to: {BEST_MODEL_DIR}\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # SAVE METADATA\n",
    "        # ====================================================================\n",
    "        metadata = {\n",
    "            'source_checkpoint': best_checkpoint['checkpoint'],\n",
    "            'epoch': best_checkpoint['epoch'],\n",
    "            'eval_loss': best_checkpoint['eval_loss'],\n",
    "            'step': best_checkpoint['step'],\n",
    "            'model_type': 'LoRA adapter (4-bit quantized)',\n",
    "            'usage': 'Use with AutoPeftModelForCausalLM.from_pretrained()'\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(BEST_MODEL_DIR, \"best_model_info.json\"), \"w\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"✓ Metadata saved to {BEST_MODEL_DIR}/best_model_info.json\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # LIST FILES IN BEST MODEL DIR\n",
    "        # ====================================================================\n",
    "        print(f\"\\n✓ Files in {BEST_MODEL_DIR}:\")\n",
    "        for item in os.listdir(BEST_MODEL_DIR):\n",
    "            item_path = os.path.join(BEST_MODEL_DIR, item)\n",
    "            if os.path.isfile(item_path):\n",
    "                size_mb = os.path.getsize(item_path) / (1024 * 1024)\n",
    "                print(f\"  - {item} ({size_mb:.2f} MB)\")\n",
    "            else:\n",
    "                print(f\"  - {item}/ (directory)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUCCESS!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"✓ Best model saved at: {BEST_MODEL_DIR}\")\n",
    "        print(f\"✓ Epoch: {best_checkpoint['epoch']}\")\n",
    "        print(f\"✓ Validation Loss: {best_checkpoint['eval_loss']:.4f}\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"HOW TO USE:\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"Download the entire folder: \" + BEST_MODEL_DIR)\n",
    "        print(\"\\nOn your local machine:\")\n",
    "        print(\"```python\")\n",
    "        print(\"from peft import AutoPeftModelForCausalLM\")\n",
    "        print(\"from transformers import AutoTokenizer\")\n",
    "        print()\n",
    "        print(\"# Load model\")\n",
    "        print(f\"model = AutoPeftModelForCausalLM.from_pretrained(\")\n",
    "        print(f\"    '{BEST_MODEL_DIR}',\")\n",
    "        print(f\"    device_map='auto',\")\n",
    "        print(f\"    torch_dtype='auto'\")\n",
    "        print(f\")\")\n",
    "        print()\n",
    "        print(\"# Load tokenizer\")\n",
    "        print(f\"tokenizer = AutoTokenizer.from_pretrained('{BEST_MODEL_DIR}')\")\n",
    "        print()\n",
    "        print(\"# Generate answer\")\n",
    "        print(\"inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\")\n",
    "        print(\"outputs = model.generate(**inputs, max_new_tokens=5)\")\n",
    "        print(\"answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\")\n",
    "        print(\"```\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No checkpoints with evaluation metrics found!\")\n",
    "else:\n",
    "    print(f\"❌ No checkpoints found in {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"\\n✓ Checkpoint recovery complete!\")\n",
    "print(f\"✓ Ready to download: {BEST_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "780521c9-df64-420d-8115-f184f7e7e71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Folder zipped successfully: llama2-mcq-best-with-rag.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Path to the folder you want to zip\n",
    "folder_path = \"llama2-mcq-best\"\n",
    "\n",
    "# Output zip file name (without .zip extension)\n",
    "output_zip = \"llama2-mcq-best-with-rag\"\n",
    "\n",
    "# Create the zip file\n",
    "shutil.make_archive(output_zip, 'zip', folder_path)\n",
    "\n",
    "print(f\"✅ Folder zipped successfully: {output_zip}.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2fb0ce-99cc-471d-8757-d1ed5050b930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
