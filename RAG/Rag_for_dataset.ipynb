{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a0f87e-945b-4976-ade4-5244d2d43bfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install ragatouille==0.0.8 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538393ec-9486-4d86-9918-1f67362e90d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install \"transformers==4.36.2\" \"torch>=2.1.0\" \"faiss-cpu\" --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a00f7ab0-9b82-49a1-842d-b9fa882fe14e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_125150/2835883369.py:2: UserWarning: \n",
      "********************************************************************************\n",
      "RAGatouille WARNING: Future Release Notice\n",
      "--------------------------------------------\n",
      "RAGatouille version 0.0.10 will be migrating to a PyLate backend \n",
      "instead of the current Stanford ColBERT backend.\n",
      "PyLate is a fully mature, feature-equivalent backend, that greatly facilitates compatibility.\n",
      "However, please pin version <0.0.10 if you require the Stanford ColBERT backend.\n",
      "********************************************************************************\n",
      "  from ragatouille import RAGPretrainedModel\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d05c2620-c6ec-41e8-83e4-fc207de9e278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Device: Tesla T4\n",
      "GPU Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU availability\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: GPU not detected, will use CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59b68244-0098-43af-b987-9bc9023f9649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# File paths\n",
    "CONTEXT_FILE = \"all_context.jsonl\"\n",
    "QUESTION_FILE = \"test_questions.jsonl\"  # Change to \"test.jsonl\" or \"val.jsonl\" as needed\n",
    "OUTPUT_FILE = \"test_fine_tune_with_context.jsonl\"  # Change accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13f19195-c79c-4f63-a7b5-fe34bd75965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval settings\n",
    "TOP_K = 2  # Number of contexts to retrieve\n",
    "INDEX_NAME = \"tqa_colbert_index\"  # Name for the ColBERT index\n",
    "MAX_DOC_LENGTH = 512  # Max passage size is 510 tokens, so 512 is perfect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab725d9-7119-49f2-9317-3f032a0092b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading context data...\n",
      "✓ Loaded 6810 passages from all_context.jsonl\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 1: Load Context Data\n",
    "# ============================================\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file into a list of dictionaries\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "print(\"Loading context data...\")\n",
    "contexts = load_jsonl(CONTEXT_FILE)\n",
    "print(f\"✓ Loaded {len(contexts)} passages from {CONTEXT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d218c5f4-18bf-4898-abdd-b835692bbc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing ColBERT model with GPU acceleration...\n",
      "✓ ColBERT model loaded on GPU\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 2: Initialize ColBERT Model\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nInitializing ColBERT model with GPU acceleration...\")\n",
    "# Using colbert-ir/colbertv2.0 - will automatically use GPU if available\n",
    "rag = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "print(\"✓ ColBERT model loaded on GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feea7a07-e0b9-4a65-a1a1-ae332b6d725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #just to find the max_length of the passages. No need to run\n",
    "# from transformers import AutoTokenizer\n",
    "# import statistics\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# lengths = [len(tokenizer.encode(ctx[\"content\"])) for ctx in contexts]\n",
    "# print(\"Avg tokens:\", int(statistics.mean(lengths)))\n",
    "# print(\"Max tokens:\", max(lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80f87f0a-d0d7-4e4e-b61f-c3968e41aa7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building ColBERT index 'tqa_colbert_index' on GPU...\n",
      "Note: With T4 GPU, this should take ~5-10 minutes. This is a ONE-TIME operation.\n",
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Oct 28, 20:18:29] #> Note: Output directory .ragatouille/colbert/indexes/tqa_colbert_index already exists\n",
      "\n",
      "\n",
      "[Oct 28, 20:18:29] #> Will delete 1 files already at .ragatouille/colbert/indexes/tqa_colbert_index in 20 seconds...\n",
      "[Oct 28, 20:18:49] [0] \t\t #> Encoding 6810 passages..\n",
      "[Oct 28, 20:20:02] [0] \t\t avg_doclen_est = 114.52349853515625 \t len(local_sample) = 6,810\n",
      "[Oct 28, 20:20:03] [0] \t\t Creating 8,192 partitions.\n",
      "[Oct 28, 20:20:03] [0] \t\t *Estimated* 779,905 embeddings.\n",
      "[Oct 28, 20:20:03] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/tqa_colbert_index/plan.json ..\n",
      "PyTorch-based indexing did not succeed with error: CUDA out of memory. Tried to allocate 22.61 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.66 GiB is free. Process 211519 has 7.07 GiB memory in use. Of the allocated memory 6.86 GiB is allocated by PyTorch, and 93.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ! Reverting to using FAISS and attempting again...\n",
      "________________________________________________________________________________\n",
      "WARNING! You have a GPU available, but only `faiss-cpu` is currently installed.\n",
      " This means that indexing will be slow. To make use of your GPU.\n",
      "Please install `faiss-gpu` by running:\n",
      "pip uninstall --y faiss-cpu & pip install faiss-gpu\n",
      " ________________________________________________________________________________\n",
      "Will continue with CPU indexing in 5 seconds...\n",
      "\n",
      "\n",
      "[Oct 28, 20:20:10] #> Note: Output directory .ragatouille/colbert/indexes/tqa_colbert_index already exists\n",
      "\n",
      "\n",
      "[Oct 28, 20:20:10] #> Will delete 1 files already at .ragatouille/colbert/indexes/tqa_colbert_index in 20 seconds...\n",
      "[Oct 28, 20:20:30] [0] \t\t #> Encoding 6810 passages..\n",
      "[Oct 28, 20:21:42] [0] \t\t avg_doclen_est = 114.52349853515625 \t len(local_sample) = 6,810\n",
      "[Oct 28, 20:21:42] [0] \t\t Creating 8,192 partitions.\n",
      "[Oct 28, 20:21:42] [0] \t\t *Estimated* 779,905 embeddings.\n",
      "[Oct 28, 20:21:42] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/tqa_colbert_index/plan.json ..\n",
      "Clustering 740910 points in 128D to 8192 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.11 s\n",
      "[Oct 28, 20:28:16] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 28, 20:30:44] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0.036, 0.039, 0.039, 0.035, 0.036, 0.038, 0.037, 0.036, 0.036, 0.036, 0.036, 0.036, 0.038, 0.038, 0.035, 0.037, 0.034, 0.035, 0.037, 0.035, 0.037, 0.037, 0.036, 0.036, 0.035, 0.036, 0.036, 0.038, 0.037, 0.039, 0.039, 0.04, 0.039, 0.036, 0.036, 0.035, 0.041, 0.035, 0.036, 0.046, 0.037, 0.035, 0.036, 0.037, 0.035, 0.035, 0.035, 0.04, 0.037, 0.035, 0.036, 0.036, 0.037, 0.036, 0.035, 0.036, 0.04, 0.037, 0.044, 0.035, 0.036, 0.039, 0.037, 0.04, 0.039, 0.039, 0.042, 0.038, 0.034, 0.036, 0.041, 0.035, 0.037, 0.038, 0.036, 0.039, 0.038, 0.04, 0.038, 0.039, 0.039, 0.036, 0.037, 0.039, 0.034, 0.038, 0.035, 0.039, 0.036, 0.039, 0.038, 0.041, 0.036, 0.036, 0.036, 0.038, 0.042, 0.037, 0.036, 0.037, 0.037, 0.042, 0.041, 0.037, 0.04, 0.035, 0.037, 0.037, 0.036, 0.038, 0.038, 0.038, 0.04, 0.034, 0.04, 0.036, 0.039, 0.038, 0.037, 0.037, 0.036, 0.037, 0.039, 0.038, 0.037, 0.038, 0.037, 0.035]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 28, 20:33:09] [0] \t\t #> Encoding 6810 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:11, 71.88s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 351.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 28, 20:34:21] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Oct 28, 20:34:21] #> Building the emb2pid mapping..\n",
      "[Oct 28, 20:34:21] len(emb2pid) = 779905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 8192/8192 [00:00<00:00, 55539.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 28, 20:34:21] #> Saved optimized IVF to .ragatouille/colbert/indexes/tqa_colbert_index/ivf.pid.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done indexing!\n",
      "✓ Index built and saved as 'tqa_colbert_index'\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 3: Build Index (One-time operation)\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\nBuilding ColBERT index '{INDEX_NAME}' on GPU...\")\n",
    "print(\"Note: With T4 GPU, this should take ~5-10 minutes. This is a ONE-TIME operation.\")\n",
    "\n",
    "# Extract just the content text for indexing\n",
    "documents = [ctx[\"content\"] for ctx in contexts]\n",
    "\n",
    "# Build the index with GPU acceleration\n",
    "# RAGatouille will automatically save the index for reuse\n",
    "rag.index(\n",
    "    collection=documents,\n",
    "    index_name=INDEX_NAME,\n",
    "    max_document_length=MAX_DOC_LENGTH,  # Set to 512 for your 510-token passages\n",
    "    split_documents=False  # Your passages are already chunked\n",
    ")\n",
    "print(f\"✓ Index built and saved as '{INDEX_NAME}'\")\n",
    "#gpu is not orking done on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6721908c-c3a3-4cf2-ae34-2ea11d9d19be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading questions from test_questions.jsonl...\n",
      "✓ Loaded 2512 questions\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 4: Load Questions\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\nLoading questions from {QUESTION_FILE}...\")\n",
    "questions = load_jsonl(QUESTION_FILE)\n",
    "print(f\"✓ Loaded {len(questions)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20ff09f3-a187-40eb-b2ae-ee4c96622f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving top-2 contexts for each question...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions: 100%|██████████| 2512/2512 [01:17<00:00, 32.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Retrieved contexts for all 2512 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 5: Retrieve Contexts for Each Question\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\nRetrieving top-{TOP_K} contexts for each question...\")\n",
    "\n",
    "enhanced_data = []\n",
    "\n",
    "for q_item in tqdm(questions, desc=\"Processing questions\"):\n",
    "    query = q_item[\"question\"]\n",
    "    \n",
    "    # Search for top-k relevant passages\n",
    "    results = rag.search(query, k=TOP_K)\n",
    "    \n",
    "    # Combine retrieved contexts into single string\n",
    "    # RAGatouille returns results as list of dicts with 'content' key\n",
    "    retrieved_texts = [result[\"content\"] for result in results]\n",
    "    combined_context = \"\\n\\n\".join(retrieved_texts)\n",
    "    \n",
    "    # Create enhanced entry\n",
    "    enhanced_entry = {\n",
    "        \"question\": q_item[\"question\"],\n",
    "        \"answerChoices\": q_item[\"answerChoices\"],\n",
    "        \"correctAnswer\": q_item[\"correctAnswer\"],\n",
    "        \"context\": combined_context\n",
    "    }\n",
    "    \n",
    "    enhanced_data.append(enhanced_entry)\n",
    "\n",
    "print(f\"✓ Retrieved contexts for all {len(questions)} questions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a68b46a-4b02-4a06-9655-bd68897718f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving enhanced dataset to test_fine_tune_with_context.jsonl...\n",
      "✓ Saved 2512 question-answer pairs with contexts\n",
      "\n",
      "==================================================\n",
      "PIPELINE COMPLETE!\n",
      "==================================================\n",
      "\n",
      "Output file: test_fine_tune_with_context.jsonl\n",
      "Each entry now contains:\n",
      "  - question\n",
      "  - answerChoices\n",
      "  - correctAnswer\n",
      "  - context (top-2 passages combined)\n",
      "\n",
      "To process other files:\n",
      "1. Change QUESTION_FILE = 'test.jsonl' or 'val.jsonl'\n",
      "2. Change OUTPUT_FILE accordingly\n",
      "3. Re-run the script (index reuse will be automatic)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 6: Save Enhanced Dataset\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\nSaving enhanced dataset to {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    for item in enhanced_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"✓ Saved {len(enhanced_data)} question-answer pairs with contexts\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PIPELINE COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nOutput file: {OUTPUT_FILE}\")\n",
    "print(f\"Each entry now contains:\")\n",
    "print(\"  - question\")\n",
    "print(\"  - answerChoices\")\n",
    "print(\"  - correctAnswer\")\n",
    "print(\"  - context (top-2 passages combined)\")\n",
    "print(\"\\nTo process other files:\")\n",
    "print(\"1. Change QUESTION_FILE = 'test.jsonl' or 'val.jsonl'\")\n",
    "print(\"2. Change OUTPUT_FILE accordingly\")\n",
    "print(\"3. Re-run the script (index reuse will be automatic)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d67e321d-8e47-46be-a29e-62379486564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SAMPLE OUTPUT:\n",
      "==================================================\n",
      "{\n",
      "  \"question\": \"Steps of the scientific method include all of the following except\",\n",
      "  \"answerChoices\": \"(A) doing background research.. (B) constructing a hypothesis.. (C) asking a question.. (D) proving a theory..\",\n",
      "  \"correctAnswer\": \"D\",\n",
      "  \"context\": \"The scientific method is a process used to investigate the unknown ( Figure 1.1). It is the general process of a scientific investigation. This process uses evidence and testing. Scientists use the scientific method so they can find information. A common method allows all scientists to answer questions in a similar way. Scientists who use this method can reproduce another scientists experiments. Almost all versions of the scientific method include the following steps, although some scientists do use slight variations. 1. 2. 3. 4. 5. 6. 7. Make observations. Identify a question you would like to answer based on the observation. Find out what is already known about your observation (research). Form a hypothesis. Test the hypothesis. An...\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# OPTIONAL: Preview Sample Output\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE OUTPUT:\")\n",
    "print(\"=\"*50)\n",
    "print(json.dumps(enhanced_data[0], indent=2, ensure_ascii=False)[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a051d-084a-4028-8031-b88a80a629de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
