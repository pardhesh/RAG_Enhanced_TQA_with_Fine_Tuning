{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8b0cd73-161d-4949-905d-ee7a844c5059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99f3413a-4adc-4ff4-9a56-15a35f656f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"  # or \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "new_model = \"llama-2-7b-mcq-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f5b299d-1cd4-42c8-ae47-1b4261a4367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset files (local JSONL files)\n",
    "train_file = \"train_finetune.jsonl\"\n",
    "val_file = \"valid_finetune.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "82611506-dc29-47f8-9c47-71f79f58ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training Hyperparameters\n",
    "local_rank = -1\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "weight_decay = 0.001\n",
    "num_train_epochs = 2\n",
    "max_steps = -1  # -1 means train for num_train_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62ae4a73-df0a-4d5a-8ba1-e72724640e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LoRA Parameters\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70b5a729-e914-4532-9c22-0b143d01c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization Parameters\n",
    "use_4bit = True\n",
    "use_nested_quant = False\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64663666-4771-4a6d-99fd-a73fb875cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence and Training Settings\n",
    "max_seq_length = 1024  # Increased for longer contexts\n",
    "packing = False\n",
    "gradient_checkpointing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abd0b367-3bc7-47d5-85fb-e556af4d65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision Settings\n",
    "fp16 = False\n",
    "bf16 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ff65c38-6e24-49fa-ac23-9843826ecea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and Scheduler\n",
    "optim = \"paged_adamw_32bit\"\n",
    "lr_scheduler_type = \"cosine\"\n",
    "warmup_ratio = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9a6690a-8c8a-4559-90a7-f9bd16292430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging and Saving\n",
    "group_by_length = True\n",
    "save_steps = 100\n",
    "logging_steps = 10\n",
    "output_dir = \"llama2-mcq-finetuned\"\n",
    "report_to = \"tensorboard\"\n",
    "tb_log_dir = \"llama2-mcq-finetuned/logs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96de4412-5f4f-41a9-a133-5c5e575fef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Configuration\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8520aa55-ae8c-4629-a102-c81e08ac3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEFTune (adds noise to embeddings for better generalization)\n",
    "use_neftune = True\n",
    "neftune_noise_alpha = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c38d784f-12ef-4ea0-b4f6-a8b78a520581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONFIGURATION LOADED\n",
      "================================================================================\n",
      "Model: meta-llama/Llama-2-7b-hf\n",
      "Train file: train_finetune.jsonl\n",
      "Val file: valid_finetune.jsonl\n",
      "Output dir: llama2-mcq-finetuned\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CONFIGURATION LOADED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Train file: {train_file}\")\n",
    "print(f\"Val file: {val_file}\")\n",
    "print(f\"Output dir: {output_dir}\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd8dba7a-a8cc-484c-990c-b996c2008a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets from local JSONL files...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets from local JSONL files...\")\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files=train_file, split=\"train\")\n",
    "val_dataset = load_dataset(\"json\", data_files=val_file, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc695031-3989-4bbb-943c-8a1d4d8c6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle datasets\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "val_dataset = val_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db8bbeb0-e735-4a35-a64d-8ee655c5c1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Train samples: 8653\n",
      "✓ Validation samples: 2528\n",
      "✓ Columns: ['Context', 'question', 'answerChoices', 'correctAnswer']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"✓ Train samples: {len(train_dataset)}\")\n",
    "print(f\"✓ Validation samples: {len(val_dataset)}\")\n",
    "print(f\"✓ Columns: {train_dataset.column_names}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f87e1ec1-120a-404b-8a8f-a8c202836503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE DATA:\n",
      "================================================================================\n",
      "Context (first 200 chars): Fossils are the preserved remains of animals, plants, and other organisms from the distant past. Examples of fossils include bones, teeth, and impressions. By studying fossils, evidence for evolution ...\n",
      "Question: the fossil record provides evidence for\n",
      "Answer Choices: (A) when organisms lived on earth.. (B) how some species have gone extinct.. (C) how species evolved.. (D) all of the above.\n",
      "Correct Answer: D\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a sample\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE DATA:\")\n",
    "print(\"=\"*80)\n",
    "sample = train_dataset[0]\n",
    "print(f\"Context (first 200 chars): {sample['Context'][:200]}...\")\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Answer Choices: {sample['answerChoices']}\")\n",
    "print(f\"Correct Answer: {sample['correctAnswer']}\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7ce13bdc-a9c4-46cd-aff2-1eece7a25e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD MODEL AND TOKENIZER\n",
    "# ============================================================================\n",
    "def load_model(model_name):\n",
    "    \"\"\"Load tokenizer and model with QLoRA configuration\"\"\"\n",
    "    \n",
    "    # Compute dtype\n",
    "    compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "    # Quantization Config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=use_4bit,\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=use_nested_quant,\n",
    "    )\n",
    "\n",
    "    # Check for bfloat16 support\n",
    "    if compute_dtype == torch.float16 and use_4bit:\n",
    "        major, _ = torch.cuda.get_device_capability()\n",
    "        if major >= 8:\n",
    "            print(\"=\" * 80)\n",
    "            print(\"Your GPU supports bfloat16, you can accelerate training with bf16=True\")\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "    print(f\"Loading model: {model_name}...\")\n",
    "    \n",
    "    # Load Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    # Load LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        r=lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    # Load Tokenizer\n",
    "    print(f\"Loading tokenizer: {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    print(\"✓ Model and tokenizer loaded successfully!\\n\")\n",
    "\n",
    "    return model, tokenizer, peft_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94bcc417-04d6-4163-91ce-a769c5609b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16, you can accelerate training with bf16=True\n",
      "================================================================================\n",
      "Loading model: meta-llama/Llama-2-7b-hf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a368645720fd4ef29471d6b308c6e7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: meta-llama/Llama-2-7b-hf...\n",
      "✓ Model and tokenizer loaded successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model, tokenizer, and peft config\n",
    "model, tokenizer, peft_config = load_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b6aec4-c8d8-4d9d-b5a1-310aac4478b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA FORMATTING FUNCTION\n",
    "# ============================================================================\n",
    "def formatting_func(example):\n",
    "    \"\"\"\n",
    "    Format examples for Llama2 instruction fine-tuning\n",
    "    Using Llama2-chat format with system prompt\n",
    "    \"\"\"\n",
    "    output_texts = []\n",
    "    \n",
    "    for i in range(len(example['question'])):\n",
    "        context = str(example[\"Context\"][i])\n",
    "        options = str(example[\"answerChoices\"][i])\n",
    "        question = str(example[\"question\"][i])\n",
    "        answer = str(example[\"correctAnswer\"][i])\n",
    "        \n",
    "        # Llama2-chat format with system prompt\n",
    "        template = f\"\"\"<s>[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answer must be exactly one letter corresponding to the correct option. Do not include any explanation, punctuation, or extra text.\\n<</SYS>>\n",
    "\n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        Options: {options}\n",
    "        Answer: [/INST]{answer}</s>\"\"\"\n",
    "    \n",
    "        \n",
    "        output_texts.append(template)\n",
    "    \n",
    "    return output_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1aad9297-6557-4cbb-8b06-c2ccfe32be46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA COLLATOR (For Completion Only)\n",
    "# ============================================================================\n",
    "# This ensures we only compute loss on the answer portion, not the instruction\n",
    "response_template = \"[/INST]\"\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer.encode(response_template, add_special_tokens=False)[2:], \n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28cd09d7-00e9-4bc4-9f9e-f87c78bb2538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data formatting and collator configured\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"✓ Data formatting and collator configured\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d039613f-be94-41e4-b63f-5cd15c6daa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training arguments configured\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING ARGUMENTS\n",
    "# ============================================================================\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=report_to,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    auto_find_batch_size=True,  # Automatically find optimal batch size if OOM\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "print(\"✓ Training arguments configured\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fbbe1b6a-bf28-4799-97dc-a15e343ceacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SFTTrainer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b2a0bdf9ae42bd99b5d90c50c85b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65f48193a134105bb33df7e8ceac88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using SFTConfig (TRL 1.0.0+ compatible)\n",
      "✓ Trainer initialized successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INITIALIZE TRAINER (Updated for TRL 1.0.0+ compatibility)\n",
    "# ============================================================================\n",
    "print(\"Initializing SFTTrainer...\")\n",
    "\n",
    "# Try using SFTConfig (for newer TRL versions)\n",
    "try:\n",
    "    from trl import SFTConfig\n",
    "    \n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim=optim,\n",
    "        save_steps=save_steps,\n",
    "        logging_steps=logging_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        fp16=fp16,\n",
    "        bf16=bf16,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        max_steps=max_steps,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        group_by_length=group_by_length,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        report_to=report_to,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        auto_find_batch_size=True,\n",
    "        save_total_limit=2,\n",
    "        # SFT-specific parameters\n",
    "        max_seq_length=max_seq_length,\n",
    "        packing=packing,\n",
    "        neftune_noise_alpha=neftune_noise_alpha if use_neftune else None,\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        peft_config=peft_config,\n",
    "        tokenizer=tokenizer,\n",
    "        args=sft_config,\n",
    "        data_collator=collator,\n",
    "        formatting_func=formatting_func,\n",
    "    )\n",
    "    print(\"✓ Using SFTConfig (TRL 1.0.0+ compatible)\")\n",
    "    \n",
    "except ImportError:\n",
    "    # Fallback for older TRL versions\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=packing,\n",
    "        data_collator=collator,\n",
    "        formatting_func=formatting_func,\n",
    "        neftune_noise_alpha=neftune_noise_alpha if use_neftune else None,\n",
    "    )\n",
    "    print(\"✓ Using legacy SFTTrainer initialization (pre-1.0.0)\")\n",
    "\n",
    "print(\"✓ Trainer initialized successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2964b399-e130-41d9-b851-90796aed9579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING TRAINING\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAIN THE MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a86d1b83-a708-4972-8354-a7f43113b22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25960' max='25959' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25959/25959 2:51:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.787400</td>\n",
       "      <td>2.393914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.403800</td>\n",
       "      <td>2.584598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.509500</td>\n",
       "      <td>2.934590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:451\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 451\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/accelerate/utils/memory.py:136\u001b[0m, in \u001b[0;36mfind_executable_batch_size.<locals>.decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo executable batch size found, reached zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/trainer.py:1972\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1969\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   1970\u001b[0m         smp\u001b[38;5;241m.\u001b[39mbarrier()\n\u001b[0;32m-> 1972\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_best_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1974\u001b[0m \u001b[38;5;66;03m# add remaining tr_loss\u001b[39;00m\n\u001b[1;32m   1975\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_loss_scalar \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/trainer.py:2210\u001b[0m, in \u001b[0;36mTrainer._load_best_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactive_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(best_adapter_model_path) \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(best_safe_adapter_model_path):\n\u001b[0;32m-> 2210\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_model_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;66;03m# Load_adapter has no return value present, modify it when appropriate.\u001b[39;00m\n\u001b[1;32m   2212\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _IncompatibleKeys\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/peft/peft_model.py:730\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m adapters_weights \u001b[38;5;241m=\u001b[39m load_peft_weights(model_id, device\u001b[38;5;241m=\u001b[39mtorch_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs)\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# load the weights into the model\u001b[39;00m\n\u001b[0;32m--> 730\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mset_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapters_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    732\u001b[0m     (\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_device_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_device_map\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;241m.\u001b[39mintersection({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m})) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m ):\n\u001b[1;32m    736\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/peft/utils/save_and_load.py:249\u001b[0m, in \u001b[0;36mset_peft_model_state_dict\u001b[0;34m(model, peft_model_state_dict, adapter_name)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    251\u001b[0m     model\u001b[38;5;241m.\u001b[39mprompt_encoder[adapter_name]\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m    252\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m: peft_model_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]}, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:2175\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2168\u001b[0m         out \u001b[38;5;241m=\u001b[39m hook(module, incompatible_keys)\n\u001b[1;32m   2169\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m   2170\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2171\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2172\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit should be done inplace.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2173\u001b[0m         )\n\u001b[0;32m-> 2175\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[1;32m   2178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:2163\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2161\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2162\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)}\n\u001b[0;32m-> 2163\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2166\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:2163\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2161\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2162\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)}\n\u001b[0;32m-> 2163\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2166\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "    \u001b[0;31m[... skipping similar frames: Module.load_state_dict.<locals>.load at line 2163 (5 times)]\u001b[0m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:2163\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2161\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2162\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)}\n\u001b[0;32m-> 2163\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2166\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:2157\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m assign:\n\u001b[1;32m   2156\u001b[0m     local_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massign_to_params_buffers\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m assign\n\u001b[0;32m-> 2157\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_msgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:256\u001b[0m, in \u001b[0;36mLinear4bit._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m    253\u001b[0m     bias_data \u001b[38;5;241m=\u001b[39m state_dict\u001b[38;5;241m.\u001b[39mpop(prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m bias_data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParams4bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    258\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m unexpected_keys\u001b[38;5;241m.\u001b[39mextend(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:158\u001b[0m, in \u001b[0;36mParams4bit.from_state_dict\u001b[0;34m(cls, state_dict, prefix, requires_grad)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_state_dict\u001b[39m(\u001b[38;5;28mcls\u001b[39m, state_dict, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 158\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# extracting components for QuantState from state_dict\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     qs_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mKeyError\u001b[0m: 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd5916d-cfa6-4cb2-92ad-c1b87bb814ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINDING BEST CHECKPOINT\n",
      "================================================================================\n",
      "Found 2 checkpoints:\n",
      "\n",
      "  checkpoint-8653\n",
      "    Epoch: 1.0\n",
      "    Eval Loss: 2.393913984298706\n",
      "\n",
      "  checkpoint-25959\n",
      "    Epoch: 3.0\n",
      "    Eval Loss: 2.9345898628234863\n",
      "\n",
      "================================================================================\n",
      "BEST CHECKPOINT IDENTIFIED\n",
      "================================================================================\n",
      "Checkpoint: checkpoint-8653\n",
      "Epoch: 1.0\n",
      "Eval Loss: 2.393913984298706\n",
      "Path: ./llama2-mcq-finetuned/checkpoint-8653\n",
      "================================================================================\n",
      "\n",
      "Loading best checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85307832391e467da8c9d3c5782e8415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully!\n",
      "\n",
      "Saving best model to: ./llama2-mcq-best\n",
      "✓ Best model saved!\n",
      "✓ Metadata saved to ./llama2-mcq-best/best_model_info.json\n",
      "\n",
      "================================================================================\n",
      "SUCCESS!\n",
      "================================================================================\n",
      "Your best model is saved at: ./llama2-mcq-best\n",
      "\n",
      "To use it:\n",
      "  from peft import AutoPeftModelForCausalLM\n",
      "  model = AutoPeftModelForCausalLM.from_pretrained('./llama2-mcq-best')\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Recover the best model from saved checkpoints\n",
    "When load_best_model_at_end fails due to 4-bit quantization issues\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "OUTPUT_DIR = \"./llama2-mcq-finetuned\"\n",
    "BEST_MODEL_DIR = \"./llama2-mcq-best\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINDING BEST CHECKPOINT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# FIND ALL CHECKPOINTS AND THEIR METRICS\n",
    "# ============================================================================\n",
    "checkpoints_info = []\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith(\"checkpoint-\")]\n",
    "    checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
    "    \n",
    "    print(f\"Found {len(checkpoints)} checkpoints:\\n\")\n",
    "    \n",
    "    for checkpoint in checkpoints:\n",
    "        checkpoint_path = os.path.join(OUTPUT_DIR, checkpoint)\n",
    "        trainer_state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "        \n",
    "        if os.path.exists(trainer_state_path):\n",
    "            with open(trainer_state_path) as f:\n",
    "                state = json.load(f)\n",
    "                \n",
    "                # Get epoch and eval loss\n",
    "                epoch = state.get(\"epoch\")\n",
    "                \n",
    "                # Find eval loss from log history\n",
    "                eval_loss = None\n",
    "                for log_entry in reversed(state.get(\"log_history\", [])):\n",
    "                    if \"eval_loss\" in log_entry:\n",
    "                        eval_loss = log_entry[\"eval_loss\"]\n",
    "                        break\n",
    "                \n",
    "                checkpoints_info.append({\n",
    "                    'checkpoint': checkpoint,\n",
    "                    'path': checkpoint_path,\n",
    "                    'epoch': epoch,\n",
    "                    'eval_loss': eval_loss,\n",
    "                    'step': int(checkpoint.split(\"-\")[1])\n",
    "                })\n",
    "                \n",
    "                print(f\"  {checkpoint}\")\n",
    "                print(f\"    Epoch: {epoch}\")\n",
    "                print(f\"    Eval Loss: {eval_loss}\")\n",
    "                print()\n",
    "\n",
    "# ============================================================================\n",
    "# FIND BEST CHECKPOINT (LOWEST EVAL LOSS)\n",
    "# ============================================================================\n",
    "if checkpoints_info:\n",
    "    # Filter out checkpoints without eval_loss\n",
    "    valid_checkpoints = [c for c in checkpoints_info if c['eval_loss'] is not None]\n",
    "    \n",
    "    if valid_checkpoints:\n",
    "        best_checkpoint = min(valid_checkpoints, key=lambda x: x['eval_loss'])\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"BEST CHECKPOINT IDENTIFIED\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Checkpoint: {best_checkpoint['checkpoint']}\")\n",
    "        print(f\"Epoch: {best_checkpoint['epoch']}\")\n",
    "        print(f\"Eval Loss: {best_checkpoint['eval_loss']}\")\n",
    "        print(f\"Path: {best_checkpoint['path']}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # LOAD AND SAVE BEST MODEL\n",
    "        # ====================================================================\n",
    "        print(\"Loading best checkpoint...\")\n",
    "        \n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            best_checkpoint['path'],\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=\"auto\",\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Model loaded successfully!\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(best_checkpoint['path'])\n",
    "        \n",
    "        # Save to new location\n",
    "        print(f\"\\nSaving best model to: {BEST_MODEL_DIR}\")\n",
    "        model.save_pretrained(BEST_MODEL_DIR)\n",
    "        tokenizer.save_pretrained(BEST_MODEL_DIR)\n",
    "        \n",
    "        print(\"✓ Best model saved!\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'source_checkpoint': best_checkpoint['checkpoint'],\n",
    "            'epoch': best_checkpoint['epoch'],\n",
    "            'eval_loss': best_checkpoint['eval_loss'],\n",
    "            'step': best_checkpoint['step']\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(BEST_MODEL_DIR, \"best_model_info.json\"), \"w\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"✓ Metadata saved to {BEST_MODEL_DIR}/best_model_info.json\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUCCESS!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Your best model is saved at: {BEST_MODEL_DIR}\")\n",
    "        print(\"\\nTo use it:\")\n",
    "        print(\"  from peft import AutoPeftModelForCausalLM\")\n",
    "        print(f\"  model = AutoPeftModelForCausalLM.from_pretrained('{BEST_MODEL_DIR}')\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No checkpoints with evaluation metrics found!\")\n",
    "else:\n",
    "    print(f\"❌ No checkpoints found in {OUTPUT_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc705db-5f2a-4a7f-867a-50a67094bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to change the path\n",
    "# ============================================================================\n",
    "# EVALUATE THE MODEL\n",
    "# ============================================================================\n",
    "print(\"Evaluating model on validation set...\")\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "for key, value in evaluation_results.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c555d6-21fc-46db-9f07-c21ed8e6ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: MERGE AND SAVE FULL MODEL (For Inference)\n",
    "# ============================================================================\n",
    "# Note: This step requires ~14GB VRAM for Llama-2-7b\n",
    "# You can skip this and merge later on your local machine\n",
    "\n",
    "MERGE_NOW = False  # Set to True if you want to merge on cloud\n",
    "\n",
    "if MERGE_NOW:\n",
    "    print(\"Merging LoRA weights with base model for inference...\")\n",
    "    \n",
    "    # Clear VRAM\n",
    "    del model\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Reload base model in FP16\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        low_cpu_mem_usage=True,\n",
    "        return_dict=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "    \n",
    "    # Load and merge LoRA weights\n",
    "    model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "    model = model.merge_and_unload()\n",
    "    \n",
    "    # Reload tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    # Save merged model\n",
    "    merged_output_dir = f\"{output_dir}_merged\"\n",
    "    print(f\"Saving merged model to {merged_output_dir}...\")\n",
    "    model.save_pretrained(merged_output_dir)\n",
    "    tokenizer.save_pretrained(merged_output_dir)\n",
    "    \n",
    "    print(f\"✓ Merged model saved to: {merged_output_dir}\")\n",
    "else:\n",
    "    print(\"Skipping merge step. You can merge later on your local machine.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL DONE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ LoRA adapter saved to: {output_dir}\")\n",
    "print(f\"  Size: ~100-400MB (download this!)\")\n",
    "print(\"\\nTo use the model:\")\n",
    "print(\"\\n1. With LoRA adapter (recommended for now):\")\n",
    "print(f\"   from peft import AutoPeftModelForCausalLM\")\n",
    "print(f\"   model = AutoPeftModelForCausalLM.from_pretrained('{output_dir}')\")\n",
    "print(\"\\n2. Or merge later on your local machine (see code comments)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcdc297-1707-4fa4-be5c-cd0e3b7506e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not use yet.\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_mcq(model, tokenizer, dataset, max_samples=100):\n",
    "    correct = 0\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i in tqdm(range(total)):\n",
    "        example = dataset[i]\n",
    "        context = example[\"Context\"]\n",
    "        question = example[\"question\"]\n",
    "        choices = example[\"answerChoices\"]\n",
    "        correct_answer = example[\"correctAnswer\"]\n",
    "\n",
    "        prompt = f\"\"\"<s>[INST]<<SYS>>\n",
    "You are a helpful assistant that answers multiple choice questions.\n",
    "<</SYS>>\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Options:\n",
    "{choices}\n",
    "Your answers should only be the choice from the given multiple Options and not have any text after the answer is done.\n",
    "Your answer: [/INST]\"\"\"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=5)\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        pred = decoded.strip()[-1].upper()\n",
    "\n",
    "        if pred == correct_answer.upper():\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / total\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
