{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74878777-62e2-4a67-8c25-1493898897ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate Fine-tuned Llama2 on MCQ Dataset\n",
    "Using LoRA adapter directly (no merge needed)\n",
    "Handles variable number of options (2-7+)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import re\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d8a1f6-dadf-41f8-b976-8f9a59427b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING MODEL AND TOKENIZER\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Path to your trained LoRA adapter\n",
    "ADAPTER_PATH = \"llama2-mcq-best\"  # Your best model\n",
    "\n",
    "# Dataset file\n",
    "TEST_FILE = \"test_finetune.jsonl\"  # or \"test_finetune.jsonl\"\n",
    "\n",
    "# Evaluation settings\n",
    "MAX_SAMPLES = None  # Set to None to evaluate all samples\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING MODEL AND TOKENIZER\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a10c2447-6bc8-430e-b975-c58d3183d7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapter from: llama2-mcq-best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f714ebf60ae246018f1ce040934a5a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded on: cuda:0\n",
      "‚úì Model type: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD MODEL WITH LORA ADAPTER (NO MERGE!)\n",
    "# ============================================================================\n",
    "print(f\"Loading LoRA adapter from: {ADAPTER_PATH}\")\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    ADAPTER_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"‚úì Model loaded on: {model.device}\")\n",
    "print(f\"‚úì Model type: {type(model)}\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09f78567-e87d-4e9b-bfe8-380a602e17ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset from: test_finetune.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fea012bbfc944c9bd5f3600d1af6a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 2512 samples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD TEST DATASET\n",
    "# ============================================================================\n",
    "print(f\"Loading test dataset from: {TEST_FILE}\")\n",
    "test_dataset = load_dataset(\"json\", data_files=TEST_FILE, split=\"train\")\n",
    "print(f\"‚úì Loaded {len(test_dataset)} samples\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e217035-dc2b-4d63-9747-610fcf61d3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_valid_options(choices_str):\n",
    "    \"\"\"\n",
    "    Extract valid option letters from the choices string\n",
    "    Handles formats like: \"(A) text (B) text\" or \"A. text B. text\"\n",
    "    Returns a set of valid letters: {'A', 'B', 'C', ...}\n",
    "    \"\"\"\n",
    "    # Find all option letters (A-Z) that appear to be option labels\n",
    "    # Look for patterns like \"(A)\", \"A.\", \"A)\", or standalone \"A\"\n",
    "    pattern = r'\\(([A-Z])\\)|\\b([A-Z])[\\.\\)]|\\b([A-Z])\\s'\n",
    "    matches = re.findall(pattern, choices_str.upper())\n",
    "    \n",
    "    # Flatten the matches (regex groups) and remove empty strings\n",
    "    valid_options = set()\n",
    "    for match_tuple in matches:\n",
    "        for letter in match_tuple:\n",
    "            if letter:\n",
    "                valid_options.add(letter)\n",
    "    \n",
    "    # If no pattern found, try to find any uppercase letters A-Z\n",
    "    if not valid_options:\n",
    "        valid_options = set(re.findall(r'\\b([A-Z])\\b', choices_str.upper()))\n",
    "    \n",
    "    return valid_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bd5fabb-3774-4c1b-b9a4-5f47cd7ede77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_from_response(response, valid_options):\n",
    "    \"\"\"\n",
    "    Extract the answer letter from model response\n",
    "    Args:\n",
    "        response: The model's generated text\n",
    "        valid_options: Set of valid option letters for this question\n",
    "    Returns:\n",
    "        Extracted letter or None\n",
    "    \n",
    "    Priority order:\n",
    "    1. If response is just one letter ‚Üí return it\n",
    "    2. Look for first valid letter in the response\n",
    "    3. Handle common formats (A), [A], A. as fallback\n",
    "    \"\"\"\n",
    "    response_upper = response.upper().strip()\n",
    "    \n",
    "    # Method 1: Response is just a single letter (ideal case)\n",
    "    response_cleaned = re.sub(r'[^\\w]', '', response_upper)\n",
    "    if len(response_cleaned) == 1 and response_cleaned in valid_options:\n",
    "        return response_cleaned\n",
    "    \n",
    "    # Method 2: First valid letter in response (works for \"A\" or \"(A)\" or \"The answer is A\")\n",
    "    for char in response_upper:\n",
    "        if char in valid_options:\n",
    "            return char\n",
    "    \n",
    "    # Method 3: No valid option found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7933571d-f538-4835-bca8-3d60db5797b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION FUNCTION\n",
    "# ============================================================================\n",
    "def evaluate_mcq(model, tokenizer, dataset, max_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluate the model on MCQ dataset with variable number of options\n",
    "    \n",
    "    Args:\n",
    "        model: The fine-tuned model with LoRA adapter\n",
    "        tokenizer: The tokenizer\n",
    "        dataset: Dataset with Context, question, answerChoices, correctAnswer\n",
    "        max_samples: Maximum number of samples to evaluate (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: Accuracy score\n",
    "        results: List of dictionaries with predictions and ground truth\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = len(dataset) if max_samples is None else min(len(dataset), max_samples)\n",
    "    results = []\n",
    "    \n",
    "    # Track option distribution\n",
    "    option_counts = {}\n",
    "    \n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    print(f\"Evaluating on {total} samples...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for faster inference\n",
    "        for i in tqdm(range(total), desc=\"Evaluating\"):\n",
    "            example = dataset[i]\n",
    "            context = example[\"Context\"]\n",
    "            question = example[\"question\"]\n",
    "            choices = example[\"answerChoices\"]\n",
    "            correct_answer = example[\"correctAnswer\"].upper().strip()\n",
    "            \n",
    "            # Extract valid options for this specific question\n",
    "            valid_options = extract_valid_options(choices)\n",
    "            num_options = len(valid_options)\n",
    "            \n",
    "            # Track option distribution\n",
    "            option_counts[num_options] = option_counts.get(num_options, 0) + 1\n",
    "            \n",
    "            # SMART TRUNCATION: Truncate context if needed, but keep instruction intact\n",
    "            max_context_tokens = 1800  # Reserve tokens for instruction + question + options\n",
    "            \n",
    "            # Tokenize context separately to check length\n",
    "            context_tokens = tokenizer(context, add_special_tokens=False)['input_ids']\n",
    "            \n",
    "            # If context is too long, truncate it\n",
    "            if len(context_tokens) > max_context_tokens:\n",
    "                context_tokens = context_tokens[:max_context_tokens]\n",
    "                context = tokenizer.decode(context_tokens, skip_special_tokens=True)\n",
    "                context = context + \"... [context truncated]\"\n",
    "            \n",
    "            # Format prompt - same as training format but with clearer instructions\n",
    "            prompt = f\"\"\"<s>[INST]<<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answer must be exactly one letter corresponding to the correct option. Do not include any explanation, punctuation, or extra text.\n",
    "<</SYS>>\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Options: {choices}\n",
    "Answer: [/INST]\"\"\"\n",
    "            \n",
    "            # Tokenize (should not need truncation now, but keep as safety)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "            \n",
    "            # CRITICAL CHECK: Ensure [/INST] is in the tokenized input\n",
    "            decoded_check = tokenizer.decode(inputs['input_ids'][0])\n",
    "            if \"[/INST]\" not in decoded_check:\n",
    "                # Fallback: Use shorter context\n",
    "                context_short = context[:500] + \"... [truncated]\"\n",
    "                prompt = f\"\"\"<s>[INST]<<SYS>>\n",
    "You are a helpful assistant.\n",
    "<</SYS>>\n",
    "\n",
    "Context: {context_short}\n",
    "Question: {question}\n",
    "Options: {choices}\n",
    "Answer: [/INST]\"\"\"\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "            \n",
    "            # Generate\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=5,  # Very short - we only need one letter\n",
    "                temperature=0.1,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            \n",
    "            # Decode\n",
    "            decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract response (after [/INST])\n",
    "            if \"[/INST]\" in decoded:\n",
    "                response = decoded.split(\"[/INST]\")[-1].strip()\n",
    "            else:\n",
    "                # Model failed to generate proper response (likely due to truncation issues)\n",
    "                response = \"\"\n",
    "                pred = None\n",
    "                \n",
    "                # Store result with error flag\n",
    "                results.append({\n",
    "                    'index': i,\n",
    "                    'question': question,\n",
    "                    'choices': choices,\n",
    "                    'valid_options': sorted(list(valid_options)),\n",
    "                    'num_options': num_options,\n",
    "                    'correct_answer': correct_answer,\n",
    "                    'predicted_answer': None,\n",
    "                    'full_response': '[ERROR: No response generated - likely truncation issue]',\n",
    "                    'is_correct': False,\n",
    "                    'error': 'truncation'\n",
    "                })\n",
    "                \n",
    "                # Print warning for first few cases\n",
    "                if i < 10 and len([r for r in results if r.get('error') == 'truncation']) <= 3:\n",
    "                    print(f\"\\n‚ö†Ô∏è Warning: Question {i} - No valid response generated (truncation issue)\")\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            # Extract prediction using valid options for this question\n",
    "            pred = extract_answer_from_response(response, valid_options)\n",
    "            \n",
    "            # Compare with correct answer\n",
    "            is_correct = (pred == correct_answer)\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "            \n",
    "            # Store result\n",
    "            results.append({\n",
    "                'index': i,\n",
    "                'question': question,\n",
    "                'choices': choices,\n",
    "                'valid_options': sorted(list(valid_options)),\n",
    "                'num_options': num_options,\n",
    "                'correct_answer': correct_answer,\n",
    "                'predicted_answer': pred,\n",
    "                'full_response': response,\n",
    "                'is_correct': is_correct\n",
    "            })\n",
    "            \n",
    "            # Print first 5 examples for debugging\n",
    "            if i < 5:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Example {i+1}\")\n",
    "                print(f\"{'='*60}\")\n",
    "                print(f\"Question: {question[:80]}...\")\n",
    "                print(f\"Valid options: {sorted(list(valid_options))} ({num_options} options)\")\n",
    "                print(f\"Correct: {correct_answer}\")\n",
    "                print(f\"Predicted: {pred}\")\n",
    "                print(f\"Response: '{response}'\")\n",
    "                print(f\"Status: {'‚úì CORRECT' if is_correct else '‚úó WRONG'}\")\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total samples: {total}\")\n",
    "    print(f\"Correct predictions: {correct}\")\n",
    "    \n",
    "    # Count truncation errors\n",
    "    truncation_errors = sum(1 for r in results if r.get('error') == 'truncation')\n",
    "    if truncation_errors > 0:\n",
    "        print(f\"‚ö†Ô∏è Truncation errors: {truncation_errors} ({truncation_errors/total*100:.1f}%)\")\n",
    "        print(f\"   These questions were too long and couldn't be processed properly\")\n",
    "        print(f\"   Effective accuracy (excluding errors): {correct}/{total-truncation_errors} = {correct/(total-truncation_errors)*100:.1f}%\")\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(\"\\nOption Distribution:\")\n",
    "    for num_opts in sorted(option_counts.keys()):\n",
    "        count = option_counts[num_opts]\n",
    "        print(f\"  {num_opts} options: {count} questions ({count/total*100:.1f}%)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return accuracy, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86b6e4ef-33bd-4c3e-a2c9-1f56f4af0950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 2512 samples...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/2512 [00:00<?, ?it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Evaluating:   0%|          | 1/2512 [00:01<45:19,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Example 1\n",
      "============================================================\n",
      "Question: Steps of the scientific method include all of the following except...\n",
      "Valid options: ['A', 'B', 'C', 'D'] (4 options)\n",
      "Correct: D\n",
      "Predicted: D\n",
      "Response: 'D.\n",
      "Brie'\n",
      "Status: ‚úì CORRECT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 2/2512 [00:01<26:35,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Example 2\n",
      "============================================================\n",
      "Question: Why do scientists call the Big Bang a theory?...\n",
      "Valid options: ['A', 'B', 'C', 'D'] (4 options)\n",
      "Correct: C\n",
      "Predicted: C\n",
      "Response: 'C.\n",
      "Question:'\n",
      "Status: ‚úì CORRECT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 3/2512 [00:01<20:33,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Example 3\n",
      "============================================================\n",
      "Question: The data collected in an experiment should always be...\n",
      "Valid options: ['A', 'B', 'C', 'D'] (4 options)\n",
      "Correct: D\n",
      "Predicted: D\n",
      "Response: 'D.\n",
      "Question:'\n",
      "Status: ‚úì CORRECT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 4/2512 [00:02<18:03,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Example 4\n",
      "============================================================\n",
      "Question: Which of the following is not a scientific model?...\n",
      "Valid options: ['A', 'B', 'C', 'D'] (4 options)\n",
      "Correct: B\n",
      "Predicted: B\n",
      "Response: 'B. A B is'\n",
      "Status: ‚úì CORRECT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 5/2512 [00:02<16:21,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Example 5\n",
      "============================================================\n",
      "Question: If the results of an experiment disprove a hypothesis, then the...\n",
      "Valid options: ['A', 'B', 'C', 'D'] (4 options)\n",
      "Correct: D\n",
      "Predicted: B\n",
      "Response: 'B.\n",
      "Both'\n",
      "Status: ‚úó WRONG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2512/2512 [10:37<00:00,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE\n",
      "================================================================================\n",
      "Total samples: 2512\n",
      "Correct predictions: 2060\n",
      "Accuracy: 82.01%\n",
      "\n",
      "Option Distribution:\n",
      "  2 options: 912 questions (36.3%)\n",
      "  3 options: 1 questions (0.0%)\n",
      "  4 options: 1273 questions (50.7%)\n",
      "  5 options: 13 questions (0.5%)\n",
      "  6 options: 4 questions (0.2%)\n",
      "  7 options: 301 questions (12.0%)\n",
      "  8 options: 8 questions (0.3%)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RUN EVALUATION\n",
    "# ============================================================================\n",
    "accuracy, results = evaluate_mcq(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=test_dataset,\n",
    "    max_samples=MAX_SAMPLES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3f75297-f451-4fae-85a0-40498bc757b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DETAILED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Accuracy by Number of Options:\n",
      "  2 options: 753/912 = 82.57%\n",
      "  3 options: 0/1 = 0.00%\n",
      "  4 options: 1023/1273 = 80.36%\n",
      "  5 options: 4/13 = 30.77%\n",
      "  6 options: 4/4 = 100.00%\n",
      "  7 options: 269/301 = 89.37%\n",
      "  8 options: 7/8 = 87.50%\n",
      "\n",
      "================================================================================\n",
      "ERROR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Incorrect predictions: 452/2512 (18.0%)\n",
      "\n",
      "First 5 incorrect predictions:\n",
      "\n",
      "------------------------------------------------------------\n",
      "1. Question: If the results of an experiment disprove a hypothesis, then the...\n",
      "   Valid options: ['A', 'B', 'C', 'D']\n",
      "   Correct: D\n",
      "   Predicted: B\n",
      "   Response: 'B.\n",
      "Both'\n",
      "\n",
      "------------------------------------------------------------\n",
      "2. Question: Which of the following are good measures to follow when working in the field?...\n",
      "   Valid options: ['A', 'B', 'C', 'D']\n",
      "   Correct: D\n",
      "   Predicted: A\n",
      "   Response: 'A, B, C'\n",
      "\n",
      "------------------------------------------------------------\n",
      "3. Question: A theory will still remain even if conflicting data is discovered....\n",
      "   Valid options: ['A', 'B']\n",
      "   Correct: B\n",
      "   Predicted: A\n",
      "   Response: 'A.\n",
      "A theory'\n",
      "\n",
      "------------------------------------------------------------\n",
      "4. Question: You should wear a hoodie to protect your hair when you work in a science lab....\n",
      "   Valid options: ['A', 'B']\n",
      "   Correct: B\n",
      "   Predicted: A\n",
      "   Response: 'A.\n",
      "Explan'\n",
      "\n",
      "------------------------------------------------------------\n",
      "5. Question: A sand dune has a gentle slope on the slip face side....\n",
      "   Valid options: ['A', 'B']\n",
      "   Correct: B\n",
      "   Predicted: A\n",
      "   Response: 'A.\n",
      "A sand'\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DETAILED ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"DETAILED ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Accuracy by number of options\n",
    "from collections import defaultdict\n",
    "accuracy_by_options = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "\n",
    "for result in results:\n",
    "    num_opts = result['num_options']\n",
    "    accuracy_by_options[num_opts]['total'] += 1\n",
    "    if result['is_correct']:\n",
    "        accuracy_by_options[num_opts]['correct'] += 1\n",
    "\n",
    "print(\"\\nAccuracy by Number of Options:\")\n",
    "for num_opts in sorted(accuracy_by_options.keys()):\n",
    "    stats = accuracy_by_options[num_opts]\n",
    "    acc = stats['correct'] / stats['total'] if stats['total'] > 0 else 0\n",
    "    print(f\"  {num_opts} options: {stats['correct']}/{stats['total']} = {acc:.2%}\")\n",
    "\n",
    "# Show incorrect predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "incorrect = [r for r in results if not r['is_correct']]\n",
    "print(f\"\\nIncorrect predictions: {len(incorrect)}/{len(results)} ({len(incorrect)/len(results)*100:.1f}%)\")\n",
    "\n",
    "if len(incorrect) > 0:\n",
    "    print(\"\\nFirst 5 incorrect predictions:\")\n",
    "    for i, result in enumerate(incorrect[:5]):\n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(f\"{i+1}. Question: {result['question'][:100]}...\")\n",
    "        print(f\"   Valid options: {result['valid_options']}\")\n",
    "        print(f\"   Correct: {result['correct_answer']}\")\n",
    "        print(f\"   Predicted: {result['predicted_answer']}\")\n",
    "        print(f\"   Response: '{result['full_response']}'\")\n",
    "\n",
    "# Check for cases where model didn't produce valid options\n",
    "no_prediction = [r for r in results if r['predicted_answer'] is None]\n",
    "if no_prediction:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: {len(no_prediction)} cases where no valid option was extracted\")\n",
    "    print(\"First 3 cases:\")\n",
    "    for i, result in enumerate(no_prediction[:3]):\n",
    "        print(f\"\\n{i+1}. Response: '{result['full_response']}'\")\n",
    "        print(f\"   Valid options were: {result['valid_options']}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33ed5286-70cc-4f40-9bb9-3e9ab0e07486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Results saved to evaluation_results.json and evaluation_results.csv\n",
      "\n",
      "Done! üéâ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE RESULTS (OPTIONAL)\n",
    "# ============================================================================\n",
    "SAVE_RESULTS = True\n",
    "\n",
    "if SAVE_RESULTS:\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Convert sets to lists for JSON serialization\n",
    "    for result in results:\n",
    "        result['valid_options'] = sorted(list(result['valid_options']))\n",
    "    \n",
    "    # Save as JSON\n",
    "    with open(\"evaluation_results.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            'accuracy': accuracy,\n",
    "            'total_samples': len(results),\n",
    "            'correct': sum(1 for r in results if r['is_correct']),\n",
    "            'accuracy_by_options': {\n",
    "                str(k): {'correct': v['correct'], 'total': v['total'], \n",
    "                        'accuracy': v['correct']/v['total']}\n",
    "                for k, v in accuracy_by_options.items()\n",
    "            },\n",
    "            'results': results\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    # Save as CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"evaluation_test_results.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\n‚úì Results saved to evaluation_results.json and evaluation_results.csv\")\n",
    "\n",
    "print(\"\\nDone! üéâ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81bfbb-7975-4613-8cfb-b6f325efaae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
